{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "import importlib\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  not bother think would see movie great supspen...  negative\n",
      "1  careful one get mitt change way look kung fu f...  positive\n",
      "2  chili palmer tired movie know want success mus...  negative\n",
      "3  follow little know 1998 british film make budg...  positive\n",
      "4  dark angel cross huxley brave new world percys...  positive\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/movie_reviews_cleaned.csv')\n",
    "\n",
    "# take a peek at the data\n",
    "print(dataset.head())\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[:5000]\n",
    "train_sentiments = sentiments[:5000]\n",
    "test_reviews = reviews[5000:7000]\n",
    "test_sentiments = sentiments[5000:7000]\n",
    "\n",
    "# normalize datasets\n",
    "# norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "# norm_test_reviews = tn.normalize_corpus(test_reviews)\n",
    "\n",
    "norm_train_reviews = train_reviews\n",
    "norm_test_reviews = test_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (5000, 434563)  Test features shape: (2000, 434563)\n",
      "TFIDF model:> Train features shape: (5000, 434563)  Test features shape: (2000, 434563)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', tv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/canxiuzhang/anaconda3/envs/nlp_3_9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8605\n",
      "Precision: 0.8606\n",
      "Recall: 0.8605\n",
      "F1 Score: 0.8605\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.85      0.86      0.86       981\n",
      "    negative       0.87      0.86      0.86      1019\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        846      135\n",
      "        negative        144      875\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on BOW features\n",
    "\n",
    "importlib.reload(meu)\n",
    "\n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.866\n",
      "Precision: 0.8661\n",
      "Recall: 0.866\n",
      "F1 Score: 0.866\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.87      0.85      0.86       981\n",
      "    negative       0.86      0.88      0.87      1019\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        838      143\n",
      "        negative        125      894\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                               train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                               test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8405\n",
      "Precision: 0.8405\n",
      "Recall: 0.8405\n",
      "F1 Score: 0.8405\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.84      0.84      0.84       981\n",
      "    negative       0.84      0.84      0.84      1019\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        820      161\n",
      "        negative        158      861\n"
     ]
    }
   ],
   "source": [
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8815\n",
      "Precision: 0.8815\n",
      "Recall: 0.8815\n",
      "F1 Score: 0.8815\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.88      0.87      0.88       981\n",
      "    negative       0.88      0.89      0.88      1019\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        856      125\n",
      "        negative        112      907\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newer Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction class label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "num_classes=2 \n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [tokenizer.tokenize(text)\n",
    "                   for text in norm_train_reviews]\n",
    "y_tr = le.fit_transform(train_sentiments)\n",
    "y_train = keras.utils.to_categorical(y_tr, num_classes)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [tokenizer.tokenize(text)\n",
    "                   for text in norm_test_reviews]\n",
    "y_ts = le.fit_transform(test_sentiments)\n",
    "y_test = keras.utils.to_categorical(y_ts, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: ['negative' 'negative' 'negative'] \n",
      "Encoded Labels: [0 0 0] \n",
      "One hot encoded Labels:\n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print class label encoding map and encoded labels\n",
    "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_ts[:3], \n",
    "      '\\nOne hot encoded Labels:\\n', y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build word2vec model\n",
    "w2v_num_features = 500\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, vector_size=w2v_num_features, window=150,\n",
    "                                   min_count=10, sample=1e-3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    \n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=500)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [nlp(item) for item in norm_train_reviews]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "test_nlp = [nlp(item) for item in norm_test_reviews]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (5000, 500)  Test features shape: (2000, 500)\n",
      "GloVe model:> Train features shape: (5000, 96)  Test features shape: (2000, 96)\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n",
    "print('GloVe model:> Train features shape:', train_glove_features.shape, ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with deep neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Deep neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, activation='relu', input_shape=(num_input_features,)))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(512, activation='relu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    dnn_model.add(Dense(2))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 14:59:10.128574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample deep architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"309pt\" height=\"959pt\" viewBox=\"0.00 0.00 232.00 719.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.33 1.33) rotate(0) translate(4 715)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-715 228,-715 228,4 -4,4\"/>\n",
       "<!-- 140710036380544 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140710036380544</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-664.5 0,-710.5 224,-710.5 224,-664.5 0,-664.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.5\" y=\"-683.8\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"77,-664.5 77,-710.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-695.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"77,-687.5 132,-687.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5\" y=\"-672.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"132,-664.5 132,-710.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"178\" y=\"-695.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 500)]</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"132,-687.5 224,-687.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"178\" y=\"-672.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 500)]</text>\n",
       "</g>\n",
       "<!-- 140709264611504 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140709264611504</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18,-581.5 18,-627.5 206,-627.5 206,-581.5 18,-581.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-600.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-581.5 68,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-612.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-604.5 123,-604.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-581.5 123,-627.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-612.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 500)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-604.5 206,-604.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140710036380544&#45;&gt;140709264611504 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140710036380544-&gt;140709264611504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-664.37C112,-656.15 112,-646.66 112,-637.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-637.61 112,-627.61 108.5,-637.61 115.5,-637.61\"/>\n",
       "</g>\n",
       "<!-- 140709280937392 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140709280937392</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"12,-498.5 12,-544.5 212,-544.5 212,-498.5 12,-498.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-517.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dropout</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"74,-498.5 74,-544.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"101.5\" y=\"-529.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"74,-521.5 129,-521.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"101.5\" y=\"-506.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"129,-498.5 129,-544.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-529.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"129,-521.5 212,-521.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-506.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140709264611504&#45;&gt;140709280937392 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140709264611504-&gt;140709280937392</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-581.37C112,-573.15 112,-563.66 112,-554.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-554.61 112,-544.61 108.5,-554.61 115.5,-554.61\"/>\n",
       "</g>\n",
       "<!-- 140709280939744 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140709280939744</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18,-415.5 18,-461.5 206,-461.5 206,-415.5 18,-415.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-434.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-415.5 68,-461.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-438.5 123,-438.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-423.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-415.5 123,-461.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-438.5 206,-438.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-423.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140709280937392&#45;&gt;140709280939744 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140709280937392-&gt;140709280939744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-498.37C112,-490.15 112,-480.66 112,-471.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-471.61 112,-461.61 108.5,-471.61 115.5,-471.61\"/>\n",
       "</g>\n",
       "<!-- 140709281732400 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140709281732400</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"12,-332.5 12,-378.5 212,-378.5 212,-332.5 12,-332.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-351.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dropout</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"74,-332.5 74,-378.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"101.5\" y=\"-363.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"74,-355.5 129,-355.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"101.5\" y=\"-340.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"129,-332.5 129,-378.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-363.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"129,-355.5 212,-355.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-340.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140709280939744&#45;&gt;140709281732400 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140709280939744-&gt;140709281732400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-415.37C112,-407.15 112,-397.66 112,-388.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-388.61 112,-378.61 108.5,-388.61 115.5,-388.61\"/>\n",
       "</g>\n",
       "<!-- 140709282326128 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140709282326128</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18,-249.5 18,-295.5 206,-295.5 206,-249.5 18,-249.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-249.5 68,-295.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-272.5 123,-272.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-249.5 123,-295.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-272.5 206,-272.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140709281732400&#45;&gt;140709282326128 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140709281732400-&gt;140709282326128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-332.37C112,-324.15 112,-314.66 112,-305.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-305.61 112,-295.61 108.5,-305.61 115.5,-305.61\"/>\n",
       "</g>\n",
       "<!-- 140709282327376 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140709282327376</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"12,-166.5 12,-212.5 212,-212.5 212,-166.5 12,-166.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dropout</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"74,-166.5 74,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"101.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"74,-189.5 129,-189.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"101.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"129,-166.5 129,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"129,-189.5 212,-189.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140709282326128&#45;&gt;140709282327376 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140709282326128-&gt;140709282327376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-249.37C112,-241.15 112,-231.66 112,-222.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-222.61 112,-212.61 108.5,-222.61 115.5,-222.61\"/>\n",
       "</g>\n",
       "<!-- 140709280936432 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140709280936432</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18,-83.5 18,-129.5 206,-129.5 206,-83.5 18,-83.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-102.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-83.5 68,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"68,-106.5 123,-106.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-83.5 123,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 512)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"123,-106.5 206,-106.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 140709282327376&#45;&gt;140709280936432 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140709282327376-&gt;140709280936432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-166.37C112,-158.15 112,-148.66 112,-139.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-139.61 112,-129.61 108.5,-139.61 115.5,-139.61\"/>\n",
       "</g>\n",
       "<!-- 140709282351136 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140709282351136</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"13,-0.5 13,-46.5 211,-46.5 211,-0.5 13,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-19.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"87,-0.5 87,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"114.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"87,-23.5 142,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"114.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"142,-0.5 142,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"176.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 2)</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"142,-23.5 211,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"176.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 140709280936432&#45;&gt;140709282351136 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140709280936432-&gt;140709282351136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112,-83.37C112,-75.15 112,-65.66 112,-56.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"115.5,-56.61 112,-46.61 108.5,-56.61 115.5,-56.61\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "45/45 [==============================] - 3s 27ms/step - loss: 0.5057 - accuracy: 0.7553 - val_loss: 0.4687 - val_accuracy: 0.7720\n",
      "Epoch 2/5\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.4599 - accuracy: 0.7816 - val_loss: 0.4587 - val_accuracy: 0.7740\n",
      "Epoch 3/5\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.4508 - accuracy: 0.7862 - val_loss: 0.4510 - val_accuracy: 0.7840\n",
      "Epoch 4/5\n",
      "45/45 [==============================] - 2s 49ms/step - loss: 0.4475 - accuracy: 0.7869 - val_loss: 0.4551 - val_accuracy: 0.7880\n",
      "Epoch 5/5\n",
      "45/45 [==============================] - 1s 29ms/step - loss: 0.4386 - accuracy: 0.7956 - val_loss: 0.4412 - val_accuracy: 0.7820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff96edc8fa0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=5, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = w2v_dnn.predict(avg_wv_test_features)\n",
    "y_classes = np.argmax(y_pred, axis=1)\n",
    "predictions = le.inverse_transform(y_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.793\n",
      "Precision: 0.8005\n",
      "Recall: 0.793\n",
      "F1 Score: 0.7921\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.87      0.80       981\n",
      "    negative       0.85      0.72      0.78      1019\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.80      0.79      0.79      2000\n",
      "weighted avg       0.80      0.79      0.79      2000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        850      131\n",
      "        negative        283      736\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glove_dnn = construct_deepnn_architecture(num_input_features=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "45/45 [==============================] - 3s 27ms/step - loss: 0.6729 - accuracy: 0.5860 - val_loss: 0.6607 - val_accuracy: 0.5960\n",
      "Epoch 2/5\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.6494 - accuracy: 0.6167 - val_loss: 0.6488 - val_accuracy: 0.6180\n",
      "Epoch 3/5\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.6426 - accuracy: 0.6313 - val_loss: 0.6410 - val_accuracy: 0.6420\n",
      "Epoch 4/5\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.6303 - accuracy: 0.6473 - val_loss: 0.6332 - val_accuracy: 0.6340\n",
      "Epoch 5/5\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.6178 - accuracy: 0.6649 - val_loss: 0.6385 - val_accuracy: 0.6420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff973095940>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "glove_dnn.fit(train_glove_features, y_train, epochs=5, batch_size=batch_size, \n",
    "              shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = glove_dnn.predict(test_glove_features)\n",
    "\n",
    "y_classes = np.argmax(y_pred, axis=1)\n",
    "predictions = le.inverse_transform(y_classes)\n",
    "# predictions = le.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6475\n",
      "Precision: 0.6517\n",
      "Recall: 0.6475\n",
      "F1 Score: 0.6437\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.67      0.54      0.60       981\n",
      "    negative       0.63      0.75      0.68      1019\n",
      "\n",
      "    accuracy                           0.65      2000\n",
      "   macro avg       0.65      0.65      0.64      2000\n",
      "weighted avg       0.65      0.65      0.64      2000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        534      447\n",
      "        negative        258      761\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n",
    "                                      classes=['positive', 'negative'])  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
